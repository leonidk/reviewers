{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy, scipy.stats\n",
    "#import autograd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('fall19.xlsx')\n",
    "df.Name = df.Name.map(lambda x: x.upper() if type(x) == str else _)\n",
    "df_orig = df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft=df[(df.Year ==2019) & (df.Semester == 'Fall') & (df.Dept == 'ROB')& (df.Level == 'Graduate')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[(df.Dept == 'ROB')& (df.Level == 'Graduate') & (df.Name == 'GALEOTTI, JOHN')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft.groupby('Course ID').mean()['Overall course rate'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_code_to_dept = defaultdict(lambda : defaultdict(int))\n",
    "for row in df.itertuples():\n",
    "    course_code_to_dept[row[5][:2]][row[4]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_code_to_dept = {k: sorted([(c,x) for x,c in v.items()])[-1][1] for k,v in course_code_to_dept.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    DF_EXPAND = pd.read_excel('copied_data.xlsx')\n",
    "\n",
    "    ID_lookup = {}\n",
    "    for row in df.itertuples():\n",
    "        ID_lookup[row[7]] = row[8]\n",
    "    for row in DF_EXPAND.itertuples():\n",
    "        if row[7] not in ID_lookup:\n",
    "            ID_lookup[row[7]] = row[7].lower().replace(' ','').replace(',','')\n",
    "            #print(ID_lookup[row[7]])\n",
    "    DF_EXPAND['Instructor ID'] = DF_EXPAND['Name'].apply(lambda x: ID_lookup[x])\n",
    "    df = pd.concat([df_orig,DF_EXPAND],sort=False,ignore_index=True)\n",
    "df['Course ID'] = df['Course ID'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['Course ID']== str(16697),'Course ID'] = str(16698)\n",
    "df.loc[df['Course ID']== str(16730),'Course ID'] = str(16698)\n",
    "df = df[df.Name.map(lambda x: type(x) == str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TIME'] = (df['Year']-df['Year'].min())*2 + np.array(df['Semester'] == 'Fall').astype(np.int)\n",
    "df['TIME'] = (df['TIME'] +1)\n",
    "df['TIME'] = ((df['TIME'])/df['TIME'].max())\n",
    "df.TIME = df.TIME**2\n",
    "plt.style.use('ggplot')\n",
    "plt.style.use('seaborn-white')\n",
    "plt.plot(df.groupby('Year').mean()['TIME'],lw=4)\n",
    "plt.ylabel('weight',size=20)\n",
    "plt.xlabel('year',size=20)\n",
    "plt.gca().tick_params(axis='both', which='major', labelsize=16)\n",
    "plt.gca().tick_params(axis='both', which='minor', labelsize=16)\n",
    "plt.grid(True)\n",
    "plt.title('time weights',size=24)\n",
    "plt.tight_layout()\n",
    "plt.savefig('time-bias.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = list(df['Course ID'].unique())\n",
    "reviewers = list(df['Name'].unique())\n",
    "rm = df['Overall course rate'].mean()\n",
    "rs = df['Overall course rate'].std()\n",
    "rm,rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BANNED_NAMES = ['INDEPENDENT STUDY','INDEPENDENT STUDY','SEM.','SEMN','SEMNR','SP TPCS','SPEC TOPICS','READING','SPECIAL TOPICS','PRACTICUM','INTERNSHIP','SEMINAR','PROJECT','CAPSTONE']\n",
    "#BANNED_NAMES = ['INDEPENDENT STUDY','INDEPENDENT STUDY','SEM.','SEMN','SEMNR','READING','PRACTICUM','INTERNSHIP','SEMINAR','PROJECT','CAPSTONE']\n",
    "\n",
    "valid_names = df['Course Name'].apply(lambda x: sum([_ in x for _ in BANNED_NAMES]) == 0)\n",
    "#df= df.loc[valid_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjectsD = {k:i for i,k in enumerate(subjects)}\n",
    "reviewersD = {k:i for i,k in enumerate(reviewers)}\n",
    "df.columns[4],df.columns[6],df.columns[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "n_sub = len(subjects)\n",
    "n_rev = len(reviewers)\n",
    "dataset = np.array(df)\n",
    "total_reviews = np.zeros(n_sub)\n",
    "num_reviews = np.zeros(n_sub)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(dataset.shape[0]):\n",
    "    #print(dataset[i,4],dataset[i,6],dataset[i,23])\n",
    "    dataset[i,0] = subjectsD[dataset[i,4]]\n",
    "    dataset[i,1] = reviewersD[dataset[i,6]]\n",
    "    #dataset[i,2] = (dataset[i,2]-rm)/rs\n",
    "    \n",
    "    # normalization step\n",
    "    total_reviews[dataset[i,0]] += dataset[i,23]\n",
    "    num_reviews[dataset[i,0]] += 1\n",
    "\n",
    "average_reviews = total_reviews/num_reviews\n",
    "np.random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewer_biases = np.random.randn(n_rev)\n",
    "subject_scores = np.random.random(n_sub)\n",
    "\n",
    "x0 = np.hstack([reviewer_biases,subject_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(dataset[:,0]).shape,n_sub,n_rev,np.unique(dataset[:,1]).shape,x0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_w = np.sqrt(df.iloc[:,10].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.columns[10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csize = df.iloc[:,10].unique()\n",
    "csize = np.array(sorted(csize))\n",
    "\n",
    "plt.style.use('seaborn-white')\n",
    "plt.plot(csize,np.sqrt(csize)/max_w,lw=4)\n",
    "plt.ylabel('weight',size=20)\n",
    "plt.xlabel('number of respondents',size=20)\n",
    "plt.gca().tick_params(axis='both', which='major', labelsize=16)\n",
    "plt.gca().tick_params(axis='both', which='minor', labelsize=16)\n",
    "plt.grid(True)\n",
    "plt.title('size weights',size=24)\n",
    "plt.xlim(left=0)\n",
    "plt.ylim(bottom=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('size-bias.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dept_means = df.loc[valid_names].groupby(['Dept','Level']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(df.columns).index('Level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df.columns).index('Num Respondents'),list(df.columns).index('Dept')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_bias = defaultdict(float)\n",
    "\n",
    "dept_bias_cnt = defaultdict(float)\n",
    "for row in df.loc[valid_names].itertuples():\n",
    "    #print(row)\n",
    "    key = row[4],row[9]\n",
    "    val,num = row[24],row[12]\n",
    "    #print(row[4],row[9],row[24],row[12])\n",
    "    if np.isfinite(val):\n",
    "        dept_bias[key] += val*num\n",
    "        dept_bias_cnt[key] += num\n",
    "\n",
    "for k in dept_bias:\n",
    "    dept_bias[k]/=dept_bias_cnt[k]\n",
    "#dept_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_bias = np.zeros(n_sub)\n",
    "for row in df.itertuples():\n",
    "    course_bias[subjectsD[row[5]]] = dept_bias[(row[4],row[9])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_names = sorted([_ for _ in dept_bias])\n",
    "n_depts = len(dept_names)\n",
    "n_dept_idx = {k:i for i,k in enumerate(dept_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xd = {}\n",
    "y = np.zeros(dataset.shape[0])\n",
    "bias = np.zeros(dataset.shape[0])\n",
    "for i,d in enumerate(dataset):\n",
    "    if np.isfinite(d[23]):\n",
    "        valid_bias = sum([_ in d[7] for _ in BANNED_NAMES]) ==0\n",
    "        valid_bias = max(0.01,float(valid_bias))\n",
    "        \n",
    "        is_grad = int(d[8] == 'Graduate')*.5 + .5\n",
    "        W =  valid_bias*is_grad*d[24]*np.sqrt(d[11])/max_w#np.sqrt(d[11]))\n",
    "        Xd[(i,d[0])] = W\n",
    "        Xd[(i,n_sub+d[1])] = W\n",
    "        \n",
    "        bias_idx = n_dept_idx[(d[3],d[8])]#['Overall course rate']\n",
    "        Xd[(i,n_sub+n_rev+bias_idx)] = W\n",
    "        y[i] = (d[23])*W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subjects\n",
    "n_rev,n_sub,n_depts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import scipy.sparse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = scipy.sparse.dok_matrix((dataset.shape[0],n_rev+n_sub+n_depts))\n",
    "X._update(Xd)\n",
    "X = scipy.sparse.csr_matrix(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape,y.max(),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XT = X.T\n",
    "Xt = XT @ X\n",
    "Xy = XT @ y.reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regV = np.ones(n_rev+n_sub+n_depts)*2.5e-2\n",
    "regV[n_sub:] = 5e-2\n",
    "regV[-n_depts:] = 0\n",
    "x = scipy.linalg.solve(Xt + scipy.diag(regV), Xy, assume_a='pos')\n",
    "diff = X.dot(x)[:,0] - y\n",
    "abs(diff).mean(),np.linalg.norm(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = np.mean(diff**2)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erm = scipy.linalg.inv(Xt + scipy.diag(regV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec = np.sqrt(np.diag(mse*erm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = scipy.stats.t.ppf(0.95,df=len(diff))\n",
    "CI = cv*sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(diff,100)\n",
    "plt.xlim(-.3,.3)\n",
    "plt.grid(True)\n",
    "plt.ylabel('number of examples',size=20)\n",
    "plt.xlabel('prediction error',size=20)\n",
    "plt.gca().tick_params(axis='both', which='major', labelsize=16)\n",
    "plt.gca().tick_params(axis='both', which='minor', labelsize=16)\n",
    "plt.grid(True)\n",
    "plt.title('Distribution of errors',size=24)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('prediction-error.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    xv = np.random.uniform(-6,-1,size=6)\n",
    "    xv = np.sort(xv)\n",
    "    Vs = []\n",
    "    for lr in xv:\n",
    "        regV = np.ones(n_rev+n_sub)*1e-3\n",
    "        regV[n_sub:] = (10**(lr))\n",
    "        x = scipy.linalg.solve(Xt + scipy.diag(regV), Xy, assume_a='pos')\n",
    "        diff = X.dot(x)[:,0] - y\n",
    "        Vs.append((lr,abs(diff).mean(),np.linalg.norm(diff)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    Vs = np.array(Vs)\n",
    "    plt.plot(Vs[:,0],Vs[:,1])\n",
    "    plt.figure()\n",
    "    plt.plot(Vs[:,0],Vs[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_dept_bias= x[-n_depts:,0]\n",
    "reviewer_biases = x[n_sub:-n_depts,0]\n",
    "course_offsets = x[:n_sub,0]\n",
    "\n",
    "CI_reg_dept_bias= CI[-n_depts:]\n",
    "CI_reviewer_biases = CI[n_sub:-n_depts]\n",
    "CI_course_offsets = CI[:n_sub]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df.itertuples():\n",
    "    course_bias[subjectsD[row[5]]] = reg_dept_bias[n_dept_idx[(row[4],row[9])]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_scores = course_offsets + course_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in dept_names:\n",
    "    print(n,reg_dept_bias[n_dept_idx[n]],dept_bias[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(reviewer_biases,50)\n",
    "plt.ylabel('number of examples',size=20)\n",
    "plt.xlabel('instructor bias',size=20)\n",
    "plt.gca().tick_params(axis='both', which='major', labelsize=16)\n",
    "plt.gca().tick_params(axis='both', which='minor', labelsize=16)\n",
    "plt.grid(True)\n",
    "plt.xlim(-1,1)\n",
    "plt.title('instructor offsets',size=24)\n",
    "plt.tight_layout()\n",
    "plt.savefig('pred-inst.pdf')\n",
    "\n",
    "plt.figure()\n",
    "_ = plt.hist(x[:n_sub,0],50)\n",
    "plt.ylabel('number of examples',size=20)\n",
    "plt.xlabel('course bias from department average',size=20)\n",
    "plt.gca().tick_params(axis='both', which='major', labelsize=16)\n",
    "plt.gca().tick_params(axis='both', which='minor', labelsize=16)\n",
    "plt.grid(True)\n",
    "plt.xlim(-1,1)\n",
    "plt.title('course offsets',size=24)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('pred-offset.pdf')\n",
    "\n",
    "plt.figure()\n",
    "_ = plt.hist(subject_scores,50)\n",
    "plt.ylabel('number of examples',size=20)\n",
    "plt.xlabel('predicted \"core\" course scores',size=20)\n",
    "plt.gca().tick_params(axis='both', which='major', labelsize=16)\n",
    "plt.gca().tick_params(axis='both', which='minor', labelsize=16)\n",
    "plt.grid(True)\n",
    "plt.title('subject scores',size=24)\n",
    "plt.xlim(2.5,5.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('pred-scores.pdf')\n",
    "\n",
    "plt.figure()\n",
    "_ = plt.hist(reg_dept_bias,50)\n",
    "plt.ylabel('number of examples',size=20)\n",
    "plt.xlabel('dept average',size=20)\n",
    "plt.gca().tick_params(axis='both', which='major', labelsize=16)\n",
    "plt.gca().tick_params(axis='both', which='minor', labelsize=16)\n",
    "plt.grid(True)\n",
    "plt.title('dept average scores',size=24)\n",
    "plt.xlim(3,5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('dept_bias.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_set = df[(df.Level == 'Graduate') & ((df.Dept == 'ROB') | (df.Dept == 'zzz'))]\n",
    "valid_Iids = set(base_set['Name'].unique())\n",
    "valid_Cids = set(base_set['Course ID'].unique())\n",
    "df.Dept.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for n in df[df.Dept == 'ROB']['Course Name']:\n",
    "#    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "class_lookup = defaultdict(list)\n",
    "name_lookup = defaultdict(list)\n",
    "\n",
    "for row in df.itertuples():\n",
    "    #print(row[7],'\\t',row[5],'\\t',row[4],'\\t',row[8])\n",
    "    #name_lookup[row[7]].append(row[6])\n",
    "    class_lookup[row[5]].append(row[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_bias[('MEG','Graduate')],dept_bias[('MLG','Graduate')],dept_bias[('ROB','Graduate')],dept_bias[('STA','Graduate')],dept_bias[('CS','Graduate')],dept_bias[('HCI','Graduate')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reviewers[i]\n",
    "#name_lookup\n",
    "len(reviewer_biases),len(reviewers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('{:40s}\\t\\t{}\\t{}'.format('name','bias','95%'))\n",
    "\n",
    "for i in np.argsort(reviewer_biases):\n",
    "    if reviewers[i] not in valid_Iids:\n",
    "        continue\n",
    "    print('{:40s}\\t\\t{:.2f}\\t{:.2f}'.format(reviewers[i],reviewer_biases[i],CI_reviewer_biases[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{:40s}\\t\\t{}\\t{}'.format('class','score','95%'))\n",
    "scores = []\n",
    "for i in np.argsort(subject_scores):\n",
    "    if subjects[i] not in valid_Cids:\n",
    "        continue\n",
    "    if subject_scores[i] == 0:\n",
    "        continue\n",
    "    scores.append(subject_scores[i])\n",
    "    print('{:40s}\\t\\t{:.2f}\\t{:.2f}'.format(class_lookup[subjects[i]][0],subject_scores[i],CI_course_offsets[i]))\n",
    "scores = np.array(scores)\n",
    "scores.mean(),scores.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "if False:\n",
    "    plt.figure(figsize=(8,8))\n",
    "    #plt.scatter(average_reviews,subject_scores+(average_reviews.mean()-subject_scores.mean()))\n",
    "    plt.scatter(average_reviews,subject_scores)\n",
    "    plt.xlabel('average review')\n",
    "    plt.ylabel('true review')\n",
    "    maxv = max(abs(average_reviews).max(),abs(subject_scores).max())\n",
    "    #plt.xlim(-maxv,maxv)\n",
    "    #plt.ylim(-maxv,maxv)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_lookup = defaultdict(list)\n",
    "for name in set(reviewers):\n",
    "    if type(name) == str:\n",
    "        last = name.split(', ')[0]\n",
    "        short_lookup[last].append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = pd.read_excel('courses_of_interest.xlsx') #courses_of_interest #spring_draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "short_lookup['OTOOLE']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "total_scores_vec = df.loc[valid_names  & (df['Num Respondents'] >= 15)  ]\n",
    "for row in targets.itertuples():\n",
    "    cv = subject_scores[subjectsD[str(row[1])]] if str(row[1]) in subjectsD else 0\n",
    "    row_dept = str(row[1])\n",
    "    if cv == 0:\n",
    "        cv = {'36': dept_bias[('STA','Graduate')],'10': dept_bias[('MLG','Graduate')], '16': dept_bias[('ROB','Graduate')]}[row_dept[:2]]\n",
    "    #print(row[-1])\n",
    "    nv = []\n",
    "    for name in row[-1].split(', '):\n",
    "        lookup_name = name.upper().replace(\"'\",'')\n",
    "        list_of_names = short_lookup[lookup_name]\n",
    "    \n",
    "        if ';' in lookup_name:\n",
    "            new_name = lookup_name.replace(';',', ')\n",
    "            if new_name in reviewersD:\n",
    "                list_of_names = [new_name]\n",
    "        \n",
    "        if len(list_of_names) == 0:\n",
    "            print('didnt find ' + lookup_name)\n",
    "            print(row)\n",
    "        if len(list_of_names) > 1:\n",
    "            print('ambig ', list_of_names)\n",
    "\n",
    "        nv.append(list_of_names)\n",
    "\n",
    "    nvL = [reviewer_biases[reviewersD[_[0]]] if len(_) > 0 else 0 for _ in nv ]\n",
    "    nv2 = nv\n",
    "    nv = np.mean(nvL) if len(nvL) > 0 else 0\n",
    "    TR = cv + nv if len(nvL) >0 else 0\n",
    "    if TR <=0.1:\n",
    "        print(row)\n",
    "        continue\n",
    "    new_name = ','.join([ _.split(';')[0] if ';' in _ else _ for _ in row[-1].split(',')])\n",
    "    #print(new_name)\n",
    "    d = {'num':row[1],'name':row[2],'Instructor':new_name,'Instructor Rating':nv,'Course Rating':cv,'Total Rating':TR}\n",
    "    d['course %'] = scipy.stats.percentileofscore(subject_scores,cv)\n",
    "    d['instru %'] = scipy.stats.percentileofscore(reviewer_biases,nv)\n",
    "    d['total %'] = scipy.stats.percentileofscore(total_scores_vec['Overall course rate'].fillna(0),TR)\n",
    "    \n",
    "    CIss = CI_course_offsets[subjectsD[str(row[1])]] if str(row[1]) in subjectsD else 0.5\n",
    "    CIL = [CI_reviewer_biases[reviewersD[_[0]]] if len(_) > 0 else 0 for _ in nv2 ]\n",
    "    CIcs = np.mean(CIL) if len(CIL) > 0 else 0.5\n",
    "    bd=  CIcs+ CIss\n",
    "    d['lb'] = TR - bd\n",
    "    d['ub'] = TR + bd\n",
    "    results.append(d)\n",
    "#RES = pd.DataFrame(results)[['num','name','Instructor','Course Rating','Instructor Rating','Total Rating','course %','instru %','total %']]\n",
    "RES = pd.DataFrame(results)[['num','name','Instructor','Course Rating','Instructor Rating','Total Rating','lb','ub']]\n",
    "\n",
    "#RES = pd.DataFrame(results)[['num','name','Instructor','course %','instru %','total %']]\n",
    "RES = RES.sort_values('Total Rating',0,False)\n",
    "RES['num'] = RES['num'].astype(str)\n",
    "RES['num'] = RES['num'].apply(lambda x: x[:2] + '-' + x[2:])\n",
    "#RES.columns = ['num', 'name', 'Instructor', 'Course Estimate', 'Instructor Estimate', 'Total Estimate']\n",
    "RES.round(2).style.background_gradient(cmap='RdYlGn')\n",
    "#RES.round(0).style.background_gradient(cmap='RdYlGn',low=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RES.to_excel('output_p2.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.Name.str.contains('WHITTAKER'.upper()).fillna(False)] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Course ID'] == str(16881)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Course Name'] == 'ROBOTICS BIZ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " targets = pd.read_excel('mech-e-spring.xlsx') #courses_of_interest #spring_draft stats_clsases spring2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for row in targets.itertuples():\n",
    "    #print(row)\n",
    "    cv = subject_scores[subjectsD[str(row[1])]] if str(row[1]) in subjectsD else 0\n",
    "    row_dept = str(row[1])\n",
    "    if cv == 0:\n",
    "        cv = {'24':reg_dept_bias[n_dept_idx[('MEG','Graduate')]],\n",
    "              '36':reg_dept_bias[n_dept_idx[('STA','Graduate')]],\n",
    "              '10':reg_dept_bias[n_dept_idx[('MLG','Graduate')]], \n",
    "              '16': reg_dept_bias[n_dept_idx[('ROB','Graduate')]]}[row_dept[:2]]\n",
    "    #print(row[-1])\n",
    "    nv = []\n",
    "    for fname in row[-1].split(';'):\n",
    "        fname = fname.replace(\"'\",'')\n",
    "        sname = fname.upper().split(',')\n",
    "        first_name = sname[-1]\n",
    "        last_name = sname[0]\n",
    "        #print(first_name,last_name)\n",
    "        \n",
    "        list_of_names = short_lookup[last_name]  \n",
    "        #print(fname,list_of_names)\n",
    "        if fname.upper() in list_of_names:\n",
    "            #print(fname)\n",
    "            list_of_names = [fname.upper()]\n",
    "        #print(fname,list_of_names,len(list_of_names))\n",
    "\n",
    "        if len(list_of_names) == 0:\n",
    "            print('didnt find ' + fname)\n",
    "        if len(list_of_names) > 1:\n",
    "            print('ambig ',fname, list_of_names)\n",
    "            list_of_names = []\n",
    "        nv.append(list_of_names)\n",
    "    #print(row[-1],nv)\n",
    "    nvL = [reviewer_biases[reviewersD[_[0]]] if len(_) > 0 else 0 for _ in nv ]\n",
    "    nv = np.mean(nvL) if len(nvL) > 0 else 0\n",
    "    TR = cv + nv if len(nvL) >0 else 0\n",
    "    if TR <=0.1:\n",
    "        print(row)\n",
    "        continue\n",
    "    new_name = ', '.join([ _.split(',')[0] if ',' in _ else _ for _ in row[-1].split(';')])\n",
    "    #print(new_name)\n",
    "    #print(new_name)\n",
    "    d = {'num':row[1],'name':row[2],'Instructor':new_name,'Instructor Rating':nv,'Course Rating':cv,'Total Rating':TR}\n",
    "    d['course %'] = scipy.stats.percentileofscore(subject_scores,cv)\n",
    "    d['instru %'] = scipy.stats.percentileofscore(reviewer_biases,nv)\n",
    "    d['total %'] = scipy.stats.percentileofscore(total_scores_vec['Overall course rate'].fillna(0),TR)\n",
    "\n",
    "    results.append(d)\n",
    "#RES = pd.DataFrame(results)[['num','name','Instructor','Course Rating','Instructor Rating','Total Rating','course %','instru %','total %']]\n",
    "RES = pd.DataFrame(results)[['num','name','Instructor','Course Rating','Instructor Rating','Total Rating']]\n",
    "\n",
    "#RES = pd.DataFrame(results)[['num','name','Instructor','course %','instru %','total %']]\n",
    "RES = RES.sort_values('Total Rating',0,False)\n",
    "RES['num'] = RES['num'].astype(str)\n",
    "RES['num'] = RES['num'].apply(lambda x: x[:2] + '-' + x[2:])\n",
    "#RES.columns = ['num', 'name', 'Instructor', 'Course Estimate', 'Instructor Estimate', 'Total Estimate']\n",
    "RES.round(2).style.background_gradient(cmap='RdYlGn')\n",
    "#RES.round(0).style.background_gradient(cmap='RdYlGn',low=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_lookup['Liu'.upper()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RES.round(2).style.background_gradient(cmap='RdYlGn').to_excel('output_MESPRING.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_data = pd.read_html('fall_2019_grad.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#html_data[-3].Instructor[10].replace('  ',';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(html_data)):\n",
    "    html_data[i].dropna(subset=['Course'],inplace=True)\n",
    "    html_data[i].loc[:,'Course'] = html_data[i].Course.map(lambda x: str(int(x)) if np.isfinite(x) else str(x))\n",
    "    html_data[i].loc[:,'Instructor'] = html_data[i].Instructor.map(lambda x: x.replace('  ',';'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(html_data)):\n",
    "    html_data[i].loc[:,'Course'] = html_data[i].Course.map(lambda x: '0' + x if len(x) ==4 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {}\n",
    "for d in html_data:\n",
    "    results = []\n",
    "    DEPT_NUM = d.loc[0].Course[:2]\n",
    "    print('\\n\\nNOW DOING THE {} DEPARTMENT\\n'.format(DEPT_NUM))\n",
    "    for row in d.itertuples():\n",
    "        #print(row)\n",
    "        cv = subject_scores[subjectsD[str(row[1])]] if str(row[1]) in subjectsD else 0\n",
    "        row_dept = str(row[1])\n",
    "        if cv == 0:\n",
    "            if row_dept[:2] in course_code_to_dept:\n",
    "                cv = reg_dept_bias[n_dept_idx[(course_code_to_dept[row_dept[:2]],'Graduate')]]\n",
    "            else:\n",
    "                #print(row)\n",
    "                cv = reg_dept_bias.mean()\n",
    "        #print(row[-1])\n",
    "        nv = []\n",
    "        for fname in row[-1].split(';'):\n",
    "            fname = fname.replace(\"'\",'')\n",
    "            sname = fname.upper().split(',')\n",
    "            first_name = sname[-1]\n",
    "            last_name = sname[0]\n",
    "            #print(first_name,last_name)\n",
    "\n",
    "            list_of_names = short_lookup[last_name]  \n",
    "            #print(fname,list_of_names)\n",
    "            if fname.upper() in list_of_names:\n",
    "                #print(fname)\n",
    "                list_of_names = [fname.upper()]\n",
    "            #print(fname,list_of_names,len(list_of_names))\n",
    "\n",
    "            if len(list_of_names) == 0:\n",
    "                print('didnt find ' + fname)\n",
    "            if len(list_of_names) > 1:\n",
    "                print('ambig ',fname, list_of_names)\n",
    "                list_of_names = []\n",
    "            nv.append(list_of_names)\n",
    "        #print(row[-1],nv)\n",
    "        nvL = [reviewer_biases[reviewersD[_[0]]] if len(_) > 0 else 0 for _ in nv ]\n",
    "        nv = np.mean(nvL) if len(nvL) > 0 else 0\n",
    "        TR = cv + nv if len(nvL) >0 else 0\n",
    "        if TR <=0.1:\n",
    "            print(row)\n",
    "            continue\n",
    "        new_name = ', '.join([ _.split(',')[0] if ',' in _ else _ for _ in row[-1].split(';')])\n",
    "        #print(new_name)\n",
    "        #print(new_name)\n",
    "        d = {'num':row[1],'name':row[2],'Instructor':new_name,'Instructor Rating':nv,'Course Rating':cv,'Total Rating':TR}\n",
    "        d['course %'] = scipy.stats.percentileofscore(subject_scores,cv)\n",
    "        d['instru %'] = scipy.stats.percentileofscore(reviewer_biases,nv)\n",
    "        d['total %'] = scipy.stats.percentileofscore(total_scores_vec['Overall course rate'].fillna(0),TR)\n",
    "\n",
    "        results.append(d)\n",
    "    #RES = pd.DataFrame(results)[['num','name','Instructor','Course Rating','Instructor Rating','Total Rating','course %','instru %','total %']]\n",
    "    RES = pd.DataFrame(results)[['num','name','Instructor','Course Rating','Instructor Rating','Total Rating']]\n",
    "\n",
    "    #RES = pd.DataFrame(results)[['num','name','Instructor','course %','instru %','total %']]\n",
    "    RES = RES.sort_values('Total Rating',0,False)\n",
    "    RES['num'] = RES['num'].astype(str)\n",
    "    RES['num'] = RES['num'].apply(lambda x: x[:2] + '-' + x[2:])\n",
    "    #RES.columns = ['num', 'name', 'Instructor', 'Course Estimate', 'Instructor Estimate', 'Total Estimate']\n",
    "    all_results[DEPT_NUM] =RES\n",
    "    #RES.round(0).style.background_gradient(cmap='RdYlGn',low=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results['16'].round(2).style.background_gradient(cmap='RdYlGn').to_excel('pred_tmp.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results['16'].set_index('num')['Total Rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft['Course ID'] = dft['Course ID'].map(lambda x: str(x)[:2] + '-' + str(x)[2:] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugr = all_results['16'].set_index('num')\n",
    "debugr['actual'] = dft.groupby('Course ID').mean()['Overall course rate']\n",
    "debugr['error'] = debugr['actual'] - debugr['Total Rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debugr['updated pred'] = all_results['16'].set_index('num')['Total Rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugr.dropna().round(2).style.background_gradient(cmap='RdYlGn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(debugr['error'],5)\n",
    "plt.title('sigma: {:.2f}'.format(debugr['error'].std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugr.dropna().round(2).style.background_gradient(cmap='RdYlGn')#.to_excel('update.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
