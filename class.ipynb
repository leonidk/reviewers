{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy, scipy.stats\n",
    "#import autograd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('table.xlsx')\n",
    "df_orig = df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    DF_EXPAND = pd.read_excel('copied_data.xlsx')\n",
    "\n",
    "    ID_lookup = {}\n",
    "    for row in df.itertuples():\n",
    "        ID_lookup[row[7]] = row[8]\n",
    "    for row in DF_EXPAND.itertuples():\n",
    "        if row[7] not in ID_lookup:\n",
    "            ID_lookup[row[7]] = row[7].lower().replace(' ','').replace(',','')\n",
    "            #print(ID_lookup[row[7]])\n",
    "    DF_EXPAND['Instructor ID'] = DF_EXPAND['Name'].apply(lambda x: ID_lookup[x])\n",
    "    df = pd.concat([df_orig,DF_EXPAND],sort=False,ignore_index=True)\n",
    "df['Course ID'] = df['Course ID'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['Course ID']== str(16697),'Course ID'] = str(16698)\n",
    "df.loc[df['Course ID']== str(16730),'Course ID'] = str(16698)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TIME'] = (df['Year']-df['Year'].min())*2 + np.array(df['Semester'] == 'Fall').astype(np.int)\n",
    "df['TIME'] = (df['TIME'] +1)\n",
    "df['TIME'] = ((df['TIME'])/df['TIME'].max())\n",
    "df.TIME = df.TIME**2\n",
    "plt.style.use('ggplot')\n",
    "plt.style.use('seaborn-white')\n",
    "plt.plot(df.groupby('Year').mean()['TIME'],lw=4)\n",
    "plt.ylabel('weight',size=20)\n",
    "plt.xlabel('year',size=20)\n",
    "plt.gca().tick_params(axis='both', which='major', labelsize=16)\n",
    "plt.gca().tick_params(axis='both', which='minor', labelsize=16)\n",
    "plt.grid(True)\n",
    "plt.title('time weights',size=24)\n",
    "plt.tight_layout()\n",
    "plt.savefig('time-bias.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = list(df['Course ID'].unique())\n",
    "reviewers = list(df['Name'].unique())\n",
    "rm = df['Overall course rate'].mean()\n",
    "rs = df['Overall course rate'].std()\n",
    "rm,rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BANNED_NAMES = ['INDEPENDENT STUDY','INDEPENDENT STUDY','SEM.','SEMN','SEMNR','SP TPCS','SPEC TOPICS','READING','SPECIAL TOPICS','PRACTICUM','INTERNSHIP','SEMINAR','PROJECT','CAPSTONE']\n",
    "#BANNED_NAMES = ['INDEPENDENT STUDY','INDEPENDENT STUDY','SEM.','SEMN','SEMNR','READING','PRACTICUM','INTERNSHIP','SEMINAR','PROJECT','CAPSTONE']\n",
    "\n",
    "valid_names = df['Course Name'].apply(lambda x: sum([_ in x for _ in BANNED_NAMES]) == 0)\n",
    "#df= df.loc[valid_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjectsD = {k:i for i,k in enumerate(subjects)}\n",
    "reviewersD = {k:i for i,k in enumerate(reviewers)}\n",
    "df.columns[4],df.columns[6],df.columns[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "n_sub = len(subjects)\n",
    "n_rev = len(reviewers)\n",
    "dataset = np.array(df)\n",
    "total_reviews = np.zeros(n_sub)\n",
    "num_reviews = np.zeros(n_sub)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(dataset.shape[0]):\n",
    "    #print(dataset[i,4],dataset[i,6],dataset[i,23])\n",
    "    dataset[i,0] = subjectsD[dataset[i,4]]\n",
    "    dataset[i,1] = reviewersD[dataset[i,6]]\n",
    "    #dataset[i,2] = (dataset[i,2]-rm)/rs\n",
    "    \n",
    "    # normalization step\n",
    "    total_reviews[dataset[i,0]] += dataset[i,23]\n",
    "    num_reviews[dataset[i,0]] += 1\n",
    "\n",
    "average_reviews = total_reviews/num_reviews\n",
    "np.random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewer_biases = np.random.randn(n_rev)\n",
    "subject_scores = np.random.random(n_sub)\n",
    "\n",
    "x0 = np.hstack([reviewer_biases,subject_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(dataset[:,0]).shape,n_sub,n_rev,np.unique(dataset[:,1]).shape,x0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_w = np.sqrt(df.iloc[:,10].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.columns[10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csize = df.iloc[:,10].unique()\n",
    "csize = np.array(sorted(csize))\n",
    "\n",
    "plt.style.use('seaborn-white')\n",
    "plt.plot(csize,np.sqrt(csize)/max_w,lw=4)\n",
    "plt.ylabel('weight',size=20)\n",
    "plt.xlabel('number of respondents',size=20)\n",
    "plt.gca().tick_params(axis='both', which='major', labelsize=16)\n",
    "plt.gca().tick_params(axis='both', which='minor', labelsize=16)\n",
    "plt.grid(True)\n",
    "plt.title('size weights',size=24)\n",
    "plt.xlim(left=0)\n",
    "plt.ylim(bottom=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('size-bias.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dept_means = df.loc[valid_names].groupby(['Dept','Level']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(df.columns).index('Level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df.columns).index('Num Respondents'),list(df.columns).index('Dept')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_bias = defaultdict(float)\n",
    "\n",
    "dept_bias_cnt = defaultdict(float)\n",
    "for row in df.loc[valid_names].itertuples():\n",
    "    #print(row)\n",
    "    key = row[4],row[9]\n",
    "    val,num = row[24],row[12]\n",
    "    #print(row[4],row[9],row[24],row[12])\n",
    "    if np.isfinite(val):\n",
    "        dept_bias[key] += val*num\n",
    "        dept_bias_cnt[key] += num\n",
    "\n",
    "for k in dept_bias:\n",
    "    dept_bias[k]/=dept_bias_cnt[k]\n",
    "#dept_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_bias = np.zeros(n_sub)\n",
    "for row in df.itertuples():\n",
    "    course_bias[subjectsD[row[5]]] = dept_bias[(row[4],row[9])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xd = {}\n",
    "y = np.zeros(dataset.shape[0])\n",
    "bias = np.zeros(dataset.shape[0])\n",
    "for i,d in enumerate(dataset):\n",
    "    #print(d[23],d[7],d[8],d[11],d[23])\n",
    "    #print(d[24],d[23],d[3],d[8])\n",
    "    if np.isfinite(d[23]):\n",
    "        valid_bias = sum([_ in d[7] for _ in BANNED_NAMES]) ==0\n",
    "        valid_bias = int(valid_bias)\n",
    "        \n",
    "        is_grad = int(d[8] == 'Graduate')*.5 + .5\n",
    "        W =  is_grad*d[24]*np.sqrt(d[11])/max_w#np.sqrt(d[11]))\n",
    "        Xd[(i,d[0])] = W\n",
    "        Xd[(i,n_sub+d[1])] = W*valid_bias\n",
    "        \n",
    "        bias[i] = dept_bias[(d[3],d[8])]#['Overall course rate']\n",
    "        y[i] = (d[23]-bias[i])*W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xd = {}\n",
    "y = np.zeros(dataset.shape[0])\n",
    "bias = np.zeros(dataset.shape[0])\n",
    "for i,d in enumerate(dataset):\n",
    "\n",
    "    if np.isfinite(d[23]):\n",
    "        valid_bias = sum([_ in d[7] for _ in BANNED_NAMES]) ==0\n",
    "        valid_bias = max(0.01,float(valid_bias))\n",
    "        \n",
    "        is_grad = int(d[8] == 'Graduate')*.5 + .5\n",
    "        W =  valid_bias*is_grad*d[24]*np.sqrt(d[11])/max_w#np.sqrt(d[11]))\n",
    "        Xd[(i,d[0])] = W\n",
    "        Xd[(i,n_sub+d[1])] = W\n",
    "        \n",
    "        bias[i] = dept_bias[(d[3],d[8])]#['Overall course rate']\n",
    "        y[i] = (d[23]-bias[i])*W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subjects\n",
    "n_rev,n_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import scipy.sparse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = scipy.sparse.dok_matrix((dataset.shape[0],n_rev+n_sub))\n",
    "X._update(Xd)\n",
    "X = scipy.sparse.csr_matrix(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape,y.max(),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XT = X.T\n",
    "Xt = XT @ X\n",
    "Xy = XT @ y.reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regV = np.ones(n_rev+n_sub)*2e-2\n",
    "regV[n_sub:] = 4e-2\n",
    "x = scipy.linalg.solve(Xt + scipy.diag(regV), Xy, assume_a='pos')\n",
    "diff = X.dot(x)[:,0] - y\n",
    "abs(diff).mean(),np.linalg.norm(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(diff,100)\n",
    "plt.xlim(-.3,.3)\n",
    "plt.grid(True)\n",
    "plt.ylabel('number of examples',size=20)\n",
    "plt.xlabel('prediction error',size=20)\n",
    "plt.gca().tick_params(axis='both', which='major', labelsize=16)\n",
    "plt.gca().tick_params(axis='both', which='minor', labelsize=16)\n",
    "plt.grid(True)\n",
    "plt.title('Distribution of errors',size=24)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('prediction-error.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    xv = np.random.uniform(-6,-1,size=6)\n",
    "    xv = np.sort(xv)\n",
    "    Vs = []\n",
    "    for lr in xv:\n",
    "        regV = np.ones(n_rev+n_sub)*1e-3\n",
    "        regV[n_sub:] = (10**(lr))\n",
    "        x = scipy.linalg.solve(Xt + scipy.diag(regV), Xy, assume_a='pos')\n",
    "        diff = X.dot(x)[:,0] - y\n",
    "        Vs.append((lr,abs(diff).mean(),np.linalg.norm(diff)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    Vs = np.array(Vs)\n",
    "    plt.plot(Vs[:,0],Vs[:,1])\n",
    "    plt.figure()\n",
    "    plt.plot(Vs[:,0],Vs[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewer_biases = x[n_sub:,0]\n",
    "subject_scores = x[:n_sub,0] + course_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(reviewer_biases,100)\n",
    "plt.ylabel('number of examples',size=20)\n",
    "plt.xlabel('instructor bias',size=20)\n",
    "plt.gca().tick_params(axis='both', which='major', labelsize=16)\n",
    "plt.gca().tick_params(axis='both', which='minor', labelsize=16)\n",
    "plt.grid(True)\n",
    "plt.xlim(-1,1)\n",
    "plt.title('instructor offsets',size=24)\n",
    "plt.tight_layout()\n",
    "plt.savefig('pred-inst.pdf')\n",
    "\n",
    "plt.figure()\n",
    "_ = plt.hist(x[:n_sub,0],100)\n",
    "plt.ylabel('number of examples',size=20)\n",
    "plt.xlabel('course bias from department average',size=20)\n",
    "plt.gca().tick_params(axis='both', which='major', labelsize=16)\n",
    "plt.gca().tick_params(axis='both', which='minor', labelsize=16)\n",
    "plt.grid(True)\n",
    "plt.xlim(-1,1)\n",
    "plt.title('course offsets',size=24)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('pred-offset.pdf')\n",
    "\n",
    "plt.figure()\n",
    "_ = plt.hist(subject_scores,100)\n",
    "plt.ylabel('number of examples',size=20)\n",
    "plt.xlabel('predicted \"core\" course scores',size=20)\n",
    "plt.gca().tick_params(axis='both', which='major', labelsize=16)\n",
    "plt.gca().tick_params(axis='both', which='minor', labelsize=16)\n",
    "plt.grid(True)\n",
    "plt.title('subject scores',size=24)\n",
    "plt.xlim(2.5,5.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('pred-scores.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_set = df[(df.Level == 'Graduate') & ((df.Dept == 'ROB') | (df.Dept == 'MLG'))]\n",
    "valid_Iids = set(base_set['Name'].unique())\n",
    "valid_Cids = set(base_set['Course ID'].unique())\n",
    "df.Dept.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for n in df[df.Dept == 'ROB']['Course Name']:\n",
    "#    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "class_lookup = defaultdict(list)\n",
    "name_lookup = defaultdict(list)\n",
    "\n",
    "for row in df.itertuples():\n",
    "    #print(row[7],'\\t',row[5],'\\t',row[4],'\\t',row[8])\n",
    "    #name_lookup[row[7]].append(row[6])\n",
    "    class_lookup[row[5]].append(row[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_bias[('MLG','Graduate')],dept_bias[('ROB','Graduate')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reviewers[i]\n",
    "#name_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('{:40s}\\t\\t{}'.format('name','bias'))\n",
    "\n",
    "for i in np.argsort(reviewer_biases):\n",
    "    if reviewers[i] not in valid_Iids:\n",
    "        continue\n",
    "    print('{:40s}\\t\\t{:.2f}'.format(reviewers[i],reviewer_biases[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{:40s}\\t\\t{}'.format('class','score'))\n",
    "scores = []\n",
    "for i in np.argsort(subject_scores):\n",
    "    if subjects[i] not in valid_Cids:\n",
    "        continue\n",
    "    if subject_scores[i] == 0:\n",
    "        continue\n",
    "    scores.append(subject_scores[i])\n",
    "    print('{:40s}\\t\\t{:.2f}'.format(class_lookup[subjects[i]][0],subject_scores[i]))\n",
    "scores = np.array(scores)\n",
    "scores.mean(),scores.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "if False:\n",
    "    plt.figure(figsize=(8,8))\n",
    "    #plt.scatter(average_reviews,subject_scores+(average_reviews.mean()-subject_scores.mean()))\n",
    "    plt.scatter(average_reviews,subject_scores)\n",
    "    plt.xlabel('average review')\n",
    "    plt.ylabel('true review')\n",
    "    maxv = max(abs(average_reviews).max(),abs(subject_scores).max())\n",
    "    #plt.xlim(-maxv,maxv)\n",
    "    #plt.ylim(-maxv,maxv)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_lookup = defaultdict(list)\n",
    "for name in set(reviewers):\n",
    "    if type(name) == str:\n",
    "        last = name.split(', ')[0]\n",
    "        short_lookup[last].append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = pd.read_excel('courses_of_interest.xlsx') #courses_of_interest #spring_draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "short_lookup['OTOOLE']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "total_scores_vec = df.loc[valid_names  & (df['Num Respondents'] >= 15)  ]\n",
    "for row in targets.itertuples():\n",
    "    cv = subject_scores[subjectsD[str(row[1])]] if str(row[1]) in subjectsD else 0\n",
    "    row_dept = str(row[1])\n",
    "    if cv == 0:\n",
    "        cv = {'10': dept_bias[('MLG','Graduate')], '16': dept_bias[('ROB','Graduate')]}[row_dept[:2]]\n",
    "    #print(row[-1])\n",
    "    nv = []\n",
    "    for _ in row[-1].split(', '):\n",
    "        lookup_name = _.upper().replace(\"'\",'')\n",
    "        list_of_names = short_lookup[lookup_name]\n",
    "    \n",
    "        if ';' in lookup_name:\n",
    "            new_name = lookup_name.replace(';',', ')\n",
    "            if new_name in reviewersD:\n",
    "                list_of_names = [new_name]\n",
    "        \n",
    "        if len(list_of_names) == 0:\n",
    "            print('didnt find ' + lookup_name)\n",
    "        if len(list_of_names) > 1:\n",
    "            print('ambig ', list_of_names)\n",
    "\n",
    "        nv.append(list_of_names)\n",
    "\n",
    "    nvL = [reviewer_biases[reviewersD[_[0]]] if len(_) > 0 else 0 for _ in nv ]\n",
    "    nv = np.mean(nvL) if len(nvL) > 0 else 0\n",
    "    TR = cv + nv if len(nvL) >0 else 0\n",
    "    if TR <=0.1:\n",
    "        print(row)\n",
    "        continue\n",
    "    new_name = ','.join([ _.split(';')[0] if ';' in _ else _ for _ in row[-1].split(',')])\n",
    "    #print(new_name)\n",
    "    d = {'num':row[1],'name':row[2],'Instructor':new_name,'Instructor Rating':nv,'Course Rating':cv,'Total Rating':TR}\n",
    "    d['course %'] = scipy.stats.percentileofscore(subject_scores,cv)\n",
    "    d['instru %'] = scipy.stats.percentileofscore(reviewer_biases,nv)\n",
    "    d['total %'] = scipy.stats.percentileofscore(total_scores_vec['Overall course rate'].fillna(0),TR)\n",
    "\n",
    "    results.append(d)\n",
    "#RES = pd.DataFrame(results)[['num','name','Instructor','Course Rating','Instructor Rating','Total Rating','course %','instru %','total %']]\n",
    "RES = pd.DataFrame(results)[['num','name','Instructor','Course Rating','Instructor Rating','Total Rating']]\n",
    "\n",
    "#RES = pd.DataFrame(results)[['num','name','Instructor','course %','instru %','total %']]\n",
    "RES = RES.sort_values('Total Rating',0,False)\n",
    "RES['num'] = RES['num'].astype(str)\n",
    "RES['num'] = RES['num'].apply(lambda x: x[:2] + '-' + x[2:])\n",
    "#RES.columns = ['num', 'name', 'Instructor', 'Course Estimate', 'Instructor Estimate', 'Total Estimate']\n",
    "RES.round(2).style.background_gradient(cmap='RdYlGn')\n",
    "#RES.round(0).style.background_gradient(cmap='RdYlGn',low=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RES.to_excel('output_p2.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.Name.str.contains('WHITTAKER'.upper()).fillna(False)] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Course ID'] == str(16881)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Course Name'] == 'ROBOTICS BIZ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = pd.read_excel('spring2020.xlsx') #courses_of_interest #spring_draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "total_scores_vec = df.loc[valid_names  & (df['Num Respondents'] >= 15)  ]\n",
    "for row in targets.itertuples():\n",
    "    #print(row)\n",
    "    cv = subject_scores[subjectsD[str(row[1])]] if str(row[1]) in subjectsD else 0\n",
    "    row_dept = str(row[1])\n",
    "    if cv == 0:\n",
    "        cv = {'10': dept_bias[('MLG','Graduate')], '16': dept_bias[('ROB','Graduate')]}[row_dept[:2]]\n",
    "    #print(row[-1])\n",
    "    nv = []\n",
    "    for fname in row[-1].split(';'):\n",
    "        fname = fname.replace(\"'\",'')\n",
    "        sname = fname.upper().split(',')\n",
    "        first_name = sname[-1]\n",
    "        last_name = sname[0]\n",
    "        #print(first_name,last_name)\n",
    "        \n",
    "        list_of_names = short_lookup[last_name]  \n",
    "        #print(fname,list_of_names)\n",
    "        if fname.upper() in list_of_names:\n",
    "            #print(fname)\n",
    "            list_of_names = [fname.upper()]\n",
    "        #print(fname,list_of_names,len(list_of_names))\n",
    "\n",
    "        if len(list_of_names) == 0:\n",
    "            print('didnt find ' + fname)\n",
    "        if len(list_of_names) > 1:\n",
    "            print('ambig ',fname, list_of_names)\n",
    "            list_of_names = []\n",
    "        nv.append(list_of_names)\n",
    "    #print(row[-1],nv)\n",
    "    nvL = [reviewer_biases[reviewersD[_[0]]] if len(_) > 0 else 0 for _ in nv ]\n",
    "    nv = np.mean(nvL) if len(nvL) > 0 else 0\n",
    "    TR = cv + nv if len(nvL) >0 else 0\n",
    "    if TR <=0.1:\n",
    "        print(row)\n",
    "        continue\n",
    "    new_name = ', '.join([ _.split(',')[0] if ',' in _ else _ for _ in row[-1].split(';')])\n",
    "    #print(new_name)\n",
    "    #print(new_name)\n",
    "    d = {'num':row[1],'name':row[2],'Instructor':new_name,'Instructor Rating':nv,'Course Rating':cv,'Total Rating':TR}\n",
    "    d['course %'] = scipy.stats.percentileofscore(subject_scores,cv)\n",
    "    d['instru %'] = scipy.stats.percentileofscore(reviewer_biases,nv)\n",
    "    d['total %'] = scipy.stats.percentileofscore(total_scores_vec['Overall course rate'].fillna(0),TR)\n",
    "\n",
    "    results.append(d)\n",
    "#RES = pd.DataFrame(results)[['num','name','Instructor','Course Rating','Instructor Rating','Total Rating','course %','instru %','total %']]\n",
    "RES = pd.DataFrame(results)[['num','name','Instructor','Course Rating','Instructor Rating','Total Rating']]\n",
    "\n",
    "#RES = pd.DataFrame(results)[['num','name','Instructor','course %','instru %','total %']]\n",
    "RES = RES.sort_values('Total Rating',0,False)\n",
    "RES['num'] = RES['num'].astype(str)\n",
    "RES['num'] = RES['num'].apply(lambda x: x[:2] + '-' + x[2:])\n",
    "#RES.columns = ['num', 'name', 'Instructor', 'Course Estimate', 'Instructor Estimate', 'Total Estimate']\n",
    "RES.round(2).style.background_gradient(cmap='RdYlGn')\n",
    "#RES.round(0).style.background_gradient(cmap='RdYlGn',low=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_lookup['Liu'.upper()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RES.to_excel('output_p3.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
